\newpage 

\section{Coupling from the past}

Consider the following practical objective:
use a computer to sample a configuration of the Ising model on a finite graph.
Computers generally have access to a source of i.i.d.\ randomness,
but cannot immediately sample from complicated distributions.
We must therefore transform the i.i.d.\ randomness
into a sample from the Ising model.

Let us first consider \emph{Glauber dynamics},
which is a standard strategy for reaching this objective.
One uses the i.i.d.\ randomness
to construct a Markov chain whose invariant distribution is the Ising model
(for example, via the Metropolis--Hastings algorithm).
To take a sample, one starts at a deterministic configuration, runs the algorithm for a deterministic number of steps (say $N$),
and then takes the final state as the sample.
This strategy has two drawbacks:
\begin{itemize}
    \item The Markov chain approximates its invariant distribution;
    the sample is not ``perfect'',
    \item The number $N$ required for the desired precision is often hard to calculate.
\end{itemize}

\emph{Coupling from the past} circumvents these problems.
This section explains the basic algorithm,
and the second section discusses theoretical implications (such as ergodicity, mixing, and uniqueness of Gibbs measures).
Throughout this section, we consider the Ising model
on a finite graph $G$ at inverse temperature $\beta$.

\begin{definition}[Treshold value at a single spin]
    The distribution of $\sigma_x$ conditional on $\{\sigma_{V\setminus\{x\}}=\zeta\}$ is given by
    \[
        \P_{G,\beta}[\{\sigma_x=-\}|\{\sigma_{V\setminus\{x\}}=\zeta\}]
        =
        \tau_\beta({\textstyle\sum_{y\sim x}\zeta_x});
        \qquad
        \tau_\beta(a):=\frac{e^{-\beta a}}{2\cosh\beta a}.
    \]
    The value $\tau_\beta$ is called the \emph{treshold value}.
\end{definition}

This means that the value of $\sigma_x$ may be resampled as follows:
first sample $U\sim U([0,1])$ (the uniform distribution on the unit interval),
then set
\[
    \sigma_x:=\begin{cases}
        - &\text{if $U\leq \tau_\beta(\sum_{y\sim x}\sigma_x)$,}\\
        + &\text{if $U> \tau_\beta(\sum_{y\sim x}\sigma_x)$.}
    \end{cases}
\]

\begin{definition}[Local update map]
    For any $x\in V$ and $U\in[0,1]$, define the \emph{local update map}
    \[
        R_{x,U}:\Omega\to\Omega,\,\sigma\mapsto
        \left(
            z\mapsto \begin{cases}
                \sigma_z&\text{if $z\neq x$}\\
                - &\text{if $z=x$ and $U\leq \tau_\beta(\sum_{y\sim x}\sigma_y)$}\\
                + &\text{if $z=x$ and $U>\tau_\beta(\sum_{y\sim x}\sigma_y)$}
            \end{cases} 
        \right).
    \]
\end{definition}

Let $\P$ denote the uniform probability measure on $(x,U)\in V\times[0,1]$.
Consider the Markov chain on $\Omega$ with transition matrix
\[
    A_{\sigma',\sigma}:=\P[\{R_{x,U}(\sigma)=\sigma'\}].
\]

\begin{exercise}[Glauber dynamics]
    \begin{enumerate}
        \item     Prove that $\langle\blank\rangle_{G,\beta}$ is a probability distribution on
        $\Omega$ solving the detailed balance equations for the transition matrix $A$.
        \item Argue that the Markov chain corresponding to $A$ is acyclic and irreducible.
        \item Conclude that $\langle\blank\rangle_{G,\beta}$ is the unique limit distribution
        of the finite state Markov chain $A$.
    \end{enumerate}
\end{exercise}

Now let $((x_i,U_i))_{i\in\Z_{\leq 0}}$ denote a sequence of i.i.d.\ copies of
the uniform random variable $(x,U)\in V\times[0,1]$,
and write $\P$ for the corresponding measure.
Write $R^{-n}$ for the random composition
\[
    R^{-n}:=R_{x_0,U_0}\circ R_{x_{-1},U_{-1}}\circ\cdots\circ R_{x_{-n+1},U_{-n+1}}.
\]
Then it is easy to see that the distribution of $R^{-n}(\sigma)$
is given by
\[
    A^n\delta_\sigma.
\]
In particular, $A^n\delta_\sigma$ converges to $\langle\blank\rangle$ as $n\to\infty$
(notice that $\R^\Omega$ is a finite dimensional vector space and therefore all reasonable topologies coincide).

\begin{theorem}[Coupling from the past]
    \label{thm:coupling_from_the_past}
    Consider the sequence $((x_i,U_i))_{i\in\Z_{\leq 0}}$ in the measure $\P$ as defined above.
    Let $T\in\Z_{\geq 0}\cup\{\infty\}$ denote the random stopping time
    defined via
    \[
        T:=\inf\{n\in\Z_{\geq 0}:\text{the function $R^{-n}:\Omega\to\Omega$ is constant as a function on $\Omega$}\}.
    \]
    Suppose that $\P(\{T<\infty\})=1$.
    Then all of the following are true:
    \begin{itemize}
        \item For any $S\geq T$, the function $R^{-S}:\Omega\to\Omega$ is also constant on $\Omega$, and $R^{-S}=R^{-T}$,
        \item The distribution of function $R^{-T}$ is $\langle\blank\rangle$.
    \end{itemize}
    Here we simply write $R^{-T}$ for $R^{-T}(\sigma)$ (with $\sigma\in\Omega$ arbitrary)
    whenever $R^{-T}$ is constant.
\end{theorem}

\begin{proof}
    Fix $\sigma\in\Omega$.
    Fix a sequence $(x_i,U_i)_i$.
    If $R^{-n}$ is constant for some $n$, then
    \[
            R^{-(n+1)}=R^{-n}\circ R_{x_{-n},U_{-n}}=R^{-n}.
    \]
    In that case, we simply have
    \[
        \lim_{m\to\infty}R^{-m}(\sigma)=R^{-n}(\sigma).
    \]
    In particular, if $T<\infty$ almost surely,
    then almost surely
    \[
        \lim_{m\to\infty} R^{-m}(\sigma)=R^{-T}.
    \]
    Since the distribution of $R^{-m}(\sigma)$ converges to $\langle\blank\rangle$
    as $m\to\infty$,
    we also have $R^{-T}\sim\langle\blank\rangle$.
\end{proof}

It is clearly important that $\P(\{T<\infty\})=1$.
We invite the reader to prove this in the following exercise.
Another proof is provided later.

\begin{exercise}[Coupling from the past: convergence (generic)]
    Show that $T$ is almost surely finite
    in the context of the above theorem.
    Hint: observe that almost surely,
    there are $|\Lambda|$ consequitive entries in the sequence $((x_i,U_i))_{i\in\Z_{\leq 0}}$
    for which $x_i$ enumerates $\Lambda$ and such that $U_i\approx 0$.
\end{exercise}

The algorithm can practically be implemented in a computer as follows.
\begin{enumerate}
    \item Fix a strictly increasing sequence $(n_k)_{k\geq 0}$ of integers with $n_0=0$.
    \item Set $k=0$, and repeat the following procedure as long as $R^{-n_k}$ is \emph{not} constant:
    \begin{enumerate}
        \item Add $1$ to the counter $k$,
        \item Draw $((x_i,U_i))_{-n_k< i\leq -n_{k-1}}$ from the independent source of randomness,
        \item Calculate the map $R^{-n_k}$.
    \end{enumerate}
    \item Output the constant value $R^{-n_k}$ as our sample from $\langle\blank\rangle$.
\end{enumerate}

The algorithm has two problems:
\begin{itemize}
    \item It requires memory to record $R^{-n_k}$ or $((x_i,U_i))_{-n_k< i\leq 0}$ between iterations,
    \item It requires time to determine if $R^{-n_k}$ is constant on $\Omega$ (which has cardinal $2^{|V|}$).
\end{itemize}

It turns out that there is an easy way to get around the second problem,
thanks to \emph{monotonicity} in the model.
This monotonicity is relative to the partial ordering $\leq$
on the sample space $\Omega=\{\pm\}^V$.

\begin{lemma}[Monotonicity of Glauber dynamics]
    Consider $((x_i,U_i))_{i\in\Z_{\leq 0}}$ fixed. Then:
    \begin{enumerate}
        \item Each map $R_{x_i,U_i}:\Omega\to\Omega$ is increasing, that is, it preserves $\leq$,
        \item Each map $R^{-n}:\Omega\to\Omega$ is increasing, that is, it preserves $\leq$,
        \item $(R^{-n}(\sigma^{\max}))_n$ is decreasing in $n$, where $\sigma^{\max}\equiv+\in\Omega$ is the maximal element,
        \item $(R^{-n}(\sigma^{\min}))_n$ is increasing in $n$, where $\sigma^{\min}\equiv+\in\Omega$ is the minimal element,
        \item We have $R^{-\infty}(\sigma^{\min})\leq R^{-\infty}(\sigma^{\max})$,
        where $R^{-\infty}(\cdots)$ denotes the $n\to\infty$ limit,
        \item If  $R^{-n}(\sigma^{\min})= R^{-n}(\sigma^{\max})$, then the map $R^{-n}$ is constant.
    \end{enumerate}
\end{lemma}

\begin{proof}
    The first item can be proved by simply inspecting the definition of $R_{x_i,U_i}$.
    The second item then follows: indeed, a composition of increasing functions is increasing.
    For the third item (and similarly, the fourth item), observe that monotonicity of $R^{-n}$ implies:
    \[
        R^{-(n+1)}(\sigma^{\max})
        =
        R^{-n}(R_{x_{-n},U_{-n}}(\sigma^{\max}))
        \leq
        R^{-n}(\sigma^{\max})
    \]
    The last two items follow from monotonicity of the map $R^{-n}$ (the second item).
\end{proof}


\begin{lemma}[Coupling from the past: convergence]
    Recall the context of Theorem~\ref{thm:coupling_from_the_past}.
    Then we have
    \[
        \{T<\infty\}=\{R^{-\infty}(\sigma^{\max})=R^{-\infty}(\sigma^{\min})\},
    \]
    and this event occurs $\P$-almost surely.
\end{lemma}

\begin{proof}
    The reformulation of the event is valid thanks to the previous lemma.
    It suffices to prove that it occurs almost surely.
    Notice that the distribution of both $R^{-\infty}(\sigma^{\max})$ and $R^{-\infty}(\sigma^{\min})$
    is given by $\langle\blank\rangle$.
    Since they have the same distribution and are almost surely $\leq$-ordered in $\P$,
    they must be almost surely equal.
\end{proof}
