\section{Double random currents}

The previous section explained how correlation functions are expressed
in terms of random currents.
We also proved a basic result, namely the existence of a demagnetised phase
of the Ising model on graphs of bounded degree.

All results discussed so far concern the behaviour of the Ising model
in the off-critical regime (very large values of $\beta$, very small values of $\beta$).
Our main interest is however in the \emph{critical regimes}:
the values for $\beta$ where the model undergoes a qualitative change,
such as values in the topological boundary of the set
\[
    \{\beta\in[0,\infty):\lim_{n\to\infty}\langle\sigma_u\rangle_{\Lambda_n,\beta}^+=0\}
\]
for a given infinite graph $G$ with a reference point $u$
(as per usual, $\Lambda_n$ refers to the graph metric ball around $u$).

We shall now introduce a new tool to study correlation functions
and random currents: the \emph{switching lemma}.
In recent years this tool has proved to be instrumental in the derivation
of rigorous results on the critical
behaviour of the Ising model, especially in graph dimensions $3$ and $4$.

\begin{lemma}[Switching lemma]
    Consider the Ising model on a finite graph $G$ at inverse temperature $\beta$.
    Let $A,B,S\subset V$.
    Then for any bounded function $F:(\Z_{\geq 0})^E\to\C$,
    the following identities hold true:
    \[
        \M^A\times \M^B[F(\n+\m)\true{\n+\m\in\calE_S}]
        =
        \M^{A\Delta S}\times \M^{B\Delta S}[F(\n+\m)\true{\n+\m\in\calE_S}],
    \]
    where $A\Delta S$ denotes the symmetric difference of $A$ and $S$.

    In terms of weights, this is equivalent to
    \begin{multline}
        \sum_{\substack{\n:\:\partial\n=A\\\m:\:\partial\m=B}}
        w_\beta(\n)w_\beta(\m)
        F(\n+\m)\true{\n+\m\in\calE_S}
        \\
        =
        \sum_{\substack{\n:\:\partial\n=A\Delta S\\\m:\:\partial\m=B\Delta S}}
        w_\beta(\n)w_\beta(\m)
        F(\n+\m)\true{\n+\m\in\calE_S}.
    \end{multline}
\end{lemma}

\begin{remark}
    While the switching lemma is an extremely powerful tool,
    its statement may appear daunting at first sight.
    Let us quickly see how switching may be applied to a simple example.
    Suppose that we record cars traversing a bridge on a road.
    Blue cars appear according to a Poisson process with rate $\lambda$.
    Let $X_B$ denote the number of blue cars that passed after recording for one hour.
    Can we easily prove, without a calculation, that
    \[
        \P[X_B\in 2\Z]\geq \P[X_B\in 2\Z+1]?
    \]

    Suppose that there are also yellow cars,
    which arrive according to an independent Poisson process with rate $\lambda$.
    Let $X_Y$ denote the number of yellow cars that passed.
    Suppose that, after waiting for one hour, $X_B+X_Y=N>0$ cars passed.
    What is the \emph{conditional} probability that $X_B$ is even?

    Well, we must have $\P[X_B\in2\Z|\{X_B+X_Y=N\}]=1/2$. Indeed, by the properties of the Poisson process,
    the distribution of the cars is invariant under \emph{switching} the colour
    of the last car. If it was blue before, then we paint it yellow,
    and vice versa.
    This operation changes the parity of $X_B$
    but leaves the conditional distribution invariant: 
    hence the symmetric probability $1/2$.

    But we cannot always do the switch.
    If $X_B+X_Y=0$
    then there is no car to repaint, and also
    $X_B=0$.
    Thus, we conclude that
    \[
        \P[X_B\in 2\Z]-\P[X_B\in 2\Z+1]=\P[\{X_B+X_Y=0\}]\geq 0.
    \]
    
    Notice that we originally asked a question about blue cars,
    but introducing yellow cars allowed us to answer it.
    This is the essence of the switching lemma.

    The switching lemma is analogous to the above example:
    \begin{itemize}
        \item The product measure corresponds to the joint distribution of blue and yellow cars,
        \item The weights correspond to the rates of the Poisson processes,
        \item The function $F$ corresponds to the conditioning event $\{X_B+X_Y=N\}$,
        \item The event $\{\n+\m\in\calE_S\}$ corresponds to the event $\{X_B+X_Y>0\}$.
    \end{itemize}
\end{remark}

\begin{proof}[Proof of the switching lemma]
    By linearity of expectation,
    it suffices to consider the case that $F(\b):=\true{\b=\a}$
    for some fixed $\a\in\calE_S$.
    Our objective is then to derive the equality
    \[
        \M^A\times \M^B[\{\n+\m=\a\}]
        =
        \M^{A\Delta S}\times \M^{B\Delta S}[\{\n+\m=\a\}]
    \]
    or
    \begin{align}
        \M^2[\{\partial\n=A,\,\partial\m=B,\,\n+\m=\a\}]
        =
        \M^2[\{\partial\n=A\Delta S,\,\partial\m=B\Delta S,\,\n+\m=\a\}].
    \end{align}
    Define the probability measure
    \[
        \P:\propto\M^2[(\blank)\true{\n+\m=\a}].
    \]
    It suffices to prove that
    \begin{equation}
        \label{eq:switching_target_equation}
    \P[\{\partial\n=A,\,\partial\m=B\}]
    =
    \P[\{\partial\n=A\Delta S,\,\partial\m=B\Delta S\}].
    \end{equation}

    By going back to the definition of $\M$ in terms of $w_\beta$,
    it is straightforward to see that the pair $(\n,\m)$ has the following probability distribution under $\P$:
    \begin{itemize}
        \item The family $(\n_{uv})_{uv}$ is a family of independent random variables,
        \item The distribution of $\n_{uv}$ is $\operatorname{Binomial}(\a_{uv},1/2)$,
        \item We have $\n+\m=\a$ almost surely, which fixes the joint distribution of $(\n,\m)$.
    \end{itemize}
    In fact, we may interpret $\P$ in a different way.
    Define the \emph{multigraph}
    \[
        \calM_\a:=\{(uv,k)\in E\times\Z_{\geq 0}:\a_{uv}<k\}
    \]
    on the vertex set $V$.
    Then $\P$ is interpreted as follows:
    \begin{itemize}
        \item We let $\calK$ denote a uniformly random subset of $\calM_\a$,
        \item We let $\n_{uv}$ denote the number of multiedges in $\calK$ between $u$ and $v$,
        \item We let $\m_{uv}$ denote the number of multiedges in $\calM_\a\setminus\calK$ between $u$ and $v$.
    \end{itemize}
    Indeed, this definition of $\P$ is consistent with our previous one.

    Proving Equation~\eqref{eq:switching_target_equation} now comes down to
    proving that the number of submultigraphs $\calK\subset\calM_\a$ contributing
    to the events on the left and right, is the same.
    Let $E_S\subset E(\a)$ denote an arbitrary subset such that $\partial E_S=S$,
    and write $E_{S,0}:=E_S\times\{0\}\subset\calM_\a$.
    The existence of the set $E_S$ follows from the fact that $\a\in\calE_S$.
    The reader may now verify that the map
    \[
        \{\partial\n=A,\,\partial\m=B\}
        \to
        \{\partial\n=A\Delta S,\,\partial\m=B\Delta S\}
        ,\,
        \calK\mapsto \calK\Delta E_{S,0}
    \]
    is a bijection.
    This proves that the two sets have the same cardinality,
    and thus the same probability under the measure $\P$.
    We have now established Equation~\eqref{eq:switching_target_equation}
    and therefore the lemma.
\end{proof}

\begin{corollary}[Second Griffiths inequality]
    Consider the Ising model on a finite graph $G$ at inverse temperature $\beta$.
    Then for any $A,B\subset V$, we have
    $\langle\sigma_{A\Delta B}\rangle_{G,\beta}-\langle\sigma_A\rangle_{G,\beta}\langle\sigma_B\rangle_{G,\beta}\geq 0$.
\end{corollary}

\begin{proof}
    We have $1=\langle1\rangle=\langle\sigma_\emptyset\rangle$.
    By the previous section (for example Corollary~\ref{cor:current_representation_of_correlation_functions}),
    \[
    Z^2
    (\langle\sigma_{A\Delta B}\rangle\langle\sigma_\emptyset\rangle-\langle\sigma_A\rangle\langle\sigma_B\rangle)
    =
    2^{2|V|}
    (
    \M^{A\Delta B}\times\M^\emptyset[1]
    -
    \M^A\times\M^B[1]
    ).
    \]
    Claim that the quantity on the right is nonnegative.
    Notice that if $\partial\m= B$, then $\m\in\calE_S$,
    and therefore
    \begin{multline}
        \M^A\times\M^B[1]
        =
        \M^A\times\M^B[\true{\n+\m\in\calE_B}]
        \stackrel{\text{switch}}=
        M^{A\Delta B}\times\M^\emptyset[\true{\n+\m\in\calE_B}]
        \\
        \leq
        M^{A\Delta B}\times\M^\emptyset[1].
    \end{multline}
    This inequality implies the claim, and therefore the second Griffiths inequality.
\end{proof}

\begin{exercise}[Conditioning on equality increases the correlation functions]
    \label{exo:conditioning_equality}
    Consider the Ising model on a finite graph $G$
    at inverse temperature $\beta$, and fix some subset $A\subset V$.
    \begin{itemize}
        \item     Prove that for any two distinct vertices $u,v\in V$,
        we have
        \[
            \E_{G,\beta}[\sigma_A|\{\sigma_u=\sigma_v\}]
            \geq
            \E_{G,\beta}[\sigma_A]=\langle\sigma_A\rangle_{G,\beta}.
        \]
        \item Prove for any $X\subset Y\subset V$, we have
        \[
            \E_{G,\beta}[\sigma_A|\{\text{$\sigma$ is constant on $X$}\}]
            \leq
            \E_{G,\beta}[\sigma_A|\{\text{$\sigma$ is constant on $Y$}\}]
            .
        \]
    \end{itemize}
\end{exercise}

\begin{exercise}[The two-point function as a metric]
    Consider the Ising model on a finite graph $G$
    at inverse temperature $\beta>0$.
    Prove that $V\times V\to [0,\infty],\,
    (u,v)\mapsto-\log\langle\sigma_u\sigma_v\rangle_{G,\beta}$
    defines a metric on $V$.
\end{exercise}

\begin{definition}[Probability measures on currents]
    Consider the Ising model on a finite graph $G$
    at inverse temperature $\beta$.
    For any $A\subset V$,
    define the probability measure
    \[
        \P^A_{G,\beta}:=\frac{2^{|V|}}{Z_{G,\beta}\langle\sigma_A\rangle_{G,\beta}}\M^A_{G,\beta}.
    \]
    For any $A_1,\ldots,A_n$,
    write $\P^{A_1,\ldots,A_n}:=\P^{A_1}\times\cdots\times\P^{A_n}$.
\end{definition}

\begin{exercise}[Correlation functions in terms of sourceless currents]
    Consider the Ising model on a finite graph $G$
    at inverse temperature $\beta$.
    Prove that for any $A\subset V$,
    \[
        \langle\sigma_A\rangle^2
        =
        \P^{\emptyset,\emptyset}[\{\n+\m\in\calE_A\}].
    \]

    Observe that we can now express all correlation functions in terms of a single
    fixed probability measure on sourceless random currents.
\end{exercise}

